Milestone 4 Prompts -TDD

# Development Prompts - SSE Implementation & Testing

## Prompt A-1 — Create Failing SSE Contract Tests

**Docs to consult:** tech-spec.md §4.1 "SSE framing rules"  
**Task:** Tests only

### tests/helpers/sse_asserts.py

```python
import json

def assert_sse_short_answer(resp):
    assert resp.status_code == 200
    assert resp.headers["content-type"].startswith("text/event-stream")

    chunks = list(resp.iter_text())
    assert chunks[0].startswith("event: short_answer")
    assert chunks[-1].strip() == "event: done"

    for chunk in chunks[1:-1]:
        if chunk.startswith("data:"):
            json.loads(chunk[5:])  # must be valid JSON
```

### tests/unit/gateway/test_sse_query_contract.py

```python
from fastapi.testclient import TestClient
from gateway.app import app
from tests.helpers.sse_asserts import assert_sse_short_answer

client = TestClient(app)

def test_query_stream_sse():
    resp = client.post("/v2/query?stream=true", json={"query": "Why?"}, stream=True)
    assert_sse_short_answer(resp)
```

**Note:** No xfail – let it fail.  
**Return:** A git-diff of the two new files only.

---

## Prompt A-2 — Implement SSE Streaming for /v2/query

**Context:** Tar attached (A-1 red). Implement per tech-spec.md §4.1:

- Accept `stream=true` query param
- Use SSE framing:

event: short_answer
data: {"text":"…"}\n\n
…
event: done\n\n
```

- Keep existing JSON output when stream is false/missing
- Add helper generator `services/gateway/src/gateway/sse.py::stream_chunks(text:str)`

**Return:** Provide diffs (new + changed files) only.

---

## Prompt A-3 — Add Failing SSE Tests for /v2/ask

**Context:** Tar attached (SSE-query tests green). Add tests only

### tests/unit/gateway/test_sse_ask_contract.py

```python
from fastapi.testclient import TestClient
from gateway.app import app
from tests.helpers.sse_asserts import assert_sse_short_answer

client = TestClient(app)

def test_ask_stream_sse():
    resp = client.post("/v2/ask?stream=true", json={"anchor_id": "demo"}, stream=True)
    assert_sse_short_answer(resp)
```

**Note:** Expect failure  
**Return:** Return diff

---

## Prompt A-4 — Implement SSE Streaming for /v2/ask

**Context:** Tar attached (A-3 red). Re-use `stream_chunks` to stream `answer.short_answer` on `/v2/ask?stream=true`.

**Return:** Provide diffs – all SSE tests must pass.

---

## Prompt B-1 — Add Failing Structured-Logging Test

**Context:** Tar attached (SSE green). Create tests only

### tests/unit/gateway/test_structured_logging.py

```python
from fastapi.testclient import TestClient
from gateway.app import app
import core_logging.logger as clog

captured = {}
def _spy(self, msg, *a, **kw):  # runs on every .info()
    if msg == "intent_completed":
        captured.update(kw)

def test_structured_logging(monkeypatch):
    monkeypatch.setattr(clog.StructuredLogger, "info", _spy, raising=True)
    client = TestClient(app)
    client.post("/v2/query", json={"query": "foo"})

    assert isinstance(captured.get("function_calls"), list)
    assert isinstance(captured.get("routing_confidence"), float)
    assert captured.get("routing_model_id")
```

**Note:** No xfail. Let it fail.  
**Return:** Return diff

---

## Prompt B-2 — Implement Intent Router Stub + Log Entry

**Context:** Tar attached (B-1 red). Follow milestone_reqs_to_test_map.md → M4 logging section:

### New services/gateway/src/gateway/intent_router.py

```python
async def route_query(text:str, allow_funcs:list[str]|None=None):
    # resolve anchor_id
    from gateway.resolver import resolve_decision_text
    anchor = await resolve_decision_text(text)
    called = [f for f in ("search_similar","get_graph_neighbors") if allow_funcs and f in allow_funcs]
    return anchor["id"], called, 0.87, "router_stub_v0"
```

- Extend QueryIn to accept optional "functions" array
- In /v2/query handler:

```python
anchor_id, called, conf, model_id = await route_query(q, req.functions)
clog.get_logger("gateway").info("intent_completed",
    extra={"function_calls":called,
           "routing_confidence":conf,
           "routing_model_id":model_id})
```

**Return:** Provide diffs; structured-logging test must pass.

---

## Prompt B-3 — Add Failing Test: Router Hits Memory-API

**Context:** Tar attached (router log green). Add tests only

### tests/unit/gateway/test_intent_router_calls.py

```python
import httpx, pytest
from fastapi.testclient import TestClient
from gateway.app import app

class SpyTransport(httpx.MockTransport):
    def __init__(self):
        super().__init__(self._handler)
        self.paths = []
    def _handler(self, req: httpx.Request):
        self.paths.append(req.url.path)
        return httpx.Response(200, json={})

def test_router_calls_memory_api(monkeypatch):
    spy = SpyTransport()
    monkeypatch.setattr(httpx, "AsyncClient", lambda *a, **k: httpx.AsyncClient(transport=spy))

    client = TestClient(app)
    client.post("/v2/query",
                json={"query":"foo",
                      "functions":["search_similar","get_graph_neighbors"]})

    assert "/api/resolve/text"          in spy.paths
    assert "/api/graph/expand_candidates" in spy.paths
```

**Return:** Return diff (failing)

---

## Prompt B-4 — Make Router Call Memory-API

**Context:** Tar attached (B-3 red). In `intent_router.route_query` use `settings.memory_api_url` to POST:

- …/api/resolve/text for search_similar
- …/api/graph/expand_candidates for get_graph_neighbors

Make calls only when the function is in allow_funcs.

**Return:** Provide diffs – router-calls test must pass.

---

## Prompt C-1 — Add Failing Parametrised Schema Test

**Context:** Tar attached (router green). Create tests only

### tests/unit/gateway/test_response_schema.py

```python
import pytest, json
from fastapi.testclient import TestClient
from gateway.app import app

client = TestClient(app)

@pytest.mark.parametrize("endpoint,payload",[
    ("/v2/query", {"query":"foo"}),
    ("/v2/ask",   {"anchor_id":"demo"})
])
def test_why_decision_schema(endpoint, payload):
    r = client.post(endpoint, json=payload)
    body = r.json()
    assert {"answer","evidence","meta"}.issubset(body.keys())
    assert body["answer"]["short_answer"]
```

**Return:** Return diff (tests should fail)

---

## Prompt C-2 — Return WhyDecisionResponse@1

**Context:** Tar attached (C-1 red).

Replace the provisional matches branch in /v2/query with a call to `builder.build_why_decision_response`, exactly like /v2/ask.

Keep the new streaming path intact.

**Return:** Provide diffs – schema tests must pass.

---

## Prompt D-1 — Add Failing Deterministic-Fallback Test

**Context:** Tar attached (schema green). Tests only

### tests/unit/gateway/test_llm_stub_fallback.py

```python
from fastapi.testclient import TestClient
from gateway.app import app

def test_llm_stub_deterministic(monkeypatch):
    monkeypatch.setenv("OPENAI_DISABLED","1")
    client = TestClient(app)
    s1 = client.post("/v2/query", json={"query":"foo"}).json()["answer"]["short_answer"]
    s2 = client.post("/v2/query", json={"query":"foo"}).json()["answer"]["short_answer"]
    assert s1 == s2 and s1
```

**Return:** Return diff (failing)

---

## Prompt D-2 — Implement llm_client.summarise() Stub

**Context:** Tar attached (D-1 red).

### services/gateway/src/gateway/llm_client.py

```python
import os
from gateway.templater import deterministic_short_answer

async def summarise(prompt_envelope: dict) -> dict:
    """
    Return a WhyDecisionAnswer-shaped dict.
    When OPENAI_DISABLED=1 → deterministic fallback.
    """
    if os.getenv("OPENAI_DISABLED") == "1":
        return {"short_answer": deterministic_short_answer(prompt_envelope["evidence"])}
    raise NotImplementedError("Real LLM integration pending")
```

In `builder.build_why_decision_response`, before calling `deterministic_short_answer`, attempt `await llm_client.summarise(envelope)`; on NotImplementedError, fall back to the deterministic templater.

**Return:** Provide diffs – all tests must pass.

---

## Prompt E-1 — Polish & Docs

**Context:** Final tar attached.

- Delete code tagged `# TODO-M4-REMOVE`
- Add/refresh doc-strings in every new module
- Run `pre-commit run -a` (black, isort, ruff)

**Return:** Return a diffstat summary (lines ± per file), nothing else.





SYSTEM / BEGIN PROMPT
You are an expert Python test-triage assistant.

Context:
– A tar.gz of the repo (paths must stay exact)
– ANALYSIS.txt with pytest-vv output

Tasks:
1. Draft a root-cause analysis using **this template**:
   ### Root-Cause <n> – <one-line summary>
   Failing tests : …
   Suspect files : …
   Explanation   : …
   Fix plan      : …
   ---
2. **Self-review**: re-read your draft; if anything is missing,
   contradictory, or unsupported, rewrite the analysis before finalising.
   Make sure that the files youre referencing exist
3. End with the single line: END OF ANALYSIS

IMPORTANT: 
1. Dont assume all tests are correct - the source of truth are the project files (milestone 3 and 4)
2. You must unpack and scan the attached TAR file for this query.
3. Do not reference files that dont exist. 

SYSTEM / END PROMPT

SYSTEM / BEGIN PROMPT
You are a code-repair assistant.

Context:
– Repository archive
– analysis_report.txt from the Analyzer step

Tasks:
1. Verify every claim in each root-cause section.
   If any claim is wrong, rewrite that section inline and mark it REVISED.
2. For each validated section, emit a unified diff
   (git diff --unified) that implements the fix plan.
3. After every diff add ONE line: # commit: <concise message>

**All diffs must comply with the functional and contract requirements
of Milestone 4 (/v2/ask + /v2/query + LLM Integration).** :contentReference[oaicite:1]{index=1}

IMPORTANT: 
1. Dont assume all tests are correct - the source of truth are the project files (milestone 3 and 4)
2. You must unpack and scan the attached TAR file for this query.

Finish with: PATCHES END
SYSTEM / END PROMPT


SYSTEM / BEGIN PROMPT
You are a continuous-integration assistant.

Context:
– Repository archive that already contains the patches
– (Optional) analysis_report.txt for reference

Tasks:
1. Run `git apply --check` (skip if repo is already patched).
2. Run `pytest -q`; capture only failing tests.
3. Run `black --check .` and `ruff .`; capture any violations.
4. If **nothing** fails, output exactly: ALL GREEN
5. Otherwise output once:

### CI REPORT
Git-apply OK?       <yes/no + details>
Tests failed?       <yes/no>
Failing tests list: <first lines of each failure>
Lint/format errors? <yes/no + first 5 lines>
NEXT STEP           Re-run Prompt #1 with new failures

No extra text.
SYSTEM / END PROMPT



#### 1. Core Packages




You’re an expert in our Python/FastAPI + ArangoDB memory system. You have:

1. All mapping & milestone docs:
   - `requirements_to_milestone_mapping.md`
   - `project_development_milestones.md`
2. The existing unit tests under:
   - `tests/unit/packages/*
3. **Attached tarball** `batvault_live_snapshot.tar` at project root—this contains the **entire source code** and all test fixtures. **You must unpack and treat its contents as the authoritative codebase.**

**Task for “Core Packages” (Milestones 1–3)**  
1. **Unpack** `batvault_live_snapshot.tar` and use it as the code reference.  
2. Parse the mapping docs to identify which Milestone 1–3 requirements apply.  
3. For each requirement:
   - Confirm code implements it (✅/⚠️/❌).
   - Confirm listed tests cover it (loading fixtures from the unpacked tar).  
4. Spot missing/partial implementations, dead code, import or lint errors.  
5. Verify public APIs, Pydantic models and signatures match across packages and client imports.  
6. Propose additional unit‐test stubs (with paths into the unpacked snapshot) for uncovered or edge cases.  
7. Generate unified‐diff patches for code fixes, new test stubs/fixes (including fixture imports), or cleanup—**all against the unpacked code**.  
8. Summarize with:
   - A checklist table (Requirement → Status → Files).
   - “Issues & Gaps” bullets, including missing tests.
   - Patches in `diff --git a/... b/...` format.



#### 2. Ingest Service

You’re an expert in our Python/FastAPI + ArangoDB memory system. You have:

1. All mapping & milestone docs:
   - `requirements_to_milestone_mapping.md`
   - `project_development_milestones.md`
2. The existing unit tests under:
   - `tests/unit/services/ingest/`
3. **Attached tarball** `batvault_live_snapshot.tar`—unpack and use as your codebase and fixtures.

**Task for “Ingest” (Milestones 1–3)**  
1. **Unpack** `batvault_live_snapshot.tar`.  
2. Parse the mapping docs for Milestone 1–3 requirements.  
3. For each requirement:
   - Confirm code implements it (✅/⚠️/❌).
   - Confirm tests cover it (loading fixtures from the unpacked snapshot).  
4. Spot missing or partial implementations, test-to-code mismatches, dead code, import or lint issues.  
5. Verify imports & API usage from Core Packages (signatures, return types, side effects).  
6. Propose any additional unit‐test stubs (with snapshot paths) to cover gaps.  
7. Generate unified‐diff patches for missing implementations, test stubs/fixes, or cleanup—**against unpacked code**.  
8. Summarize with checklist table, “Issues & Gaps,” and `diff --git` patches.



#### 3. Memory API Service

You’re an expert in our Python/FastAPI + ArangoDB memory system. You have:

1. All mapping & milestone docs:
   - `requirements_to_milestone_mapping.md`
   - `project_development_milestones.md`
2. The existing unit tests under:
   - `tests/unit/services/memory_api/`
3. **Attached tarball** `batvault_live_snapshot.tar`—unpack and use as the single source of truth for code & fixtures.

**Task for “Memory API” (Milestones 1–3)**  
1. **Unpack** `batvault_live_snapshot.tar`.  
2. Parse mapping docs for applicable Milestone 1–3 requirements.  
3. For each requirement:
   - Confirm code implements it (✅/⚠️/❌).
   - Confirm tests exercise it (using unpacked fixtures).  
4. Spot missing/partial implementations, dead or import-error code, lint issues.  
5. Verify imports & API usage from Ingest and Core Packages.  
6. Propose missing or edge-case tests (with snapshot references).  
7. Generate unified-diff patches for code fixes, test stubs/fixes, or cleanup—**against unpacked code**.  
8. Summarize with checklist, “Issues & Gaps,” and `diff --git` patches.



#### 4. API Edge Service

You’re an expert in our Python/FastAPI + ArangoDB memory system. You have:

1. All mapping & milestone docs:
   - `requirements_to_milestone_mapping.md`
   - `project_development_milestones.md`
2. The existing unit tests under:
   - `tests/unit/services/api_edge/`
3. **Attached tarball** `batvault_live_snapshot.tar`—unpack and use as your authoritative code & fixtures.

**Task for “API Edge” (Milestones 1–3)**  
1. **Unpack** `batvault_live_snapshot.tar`.  
2. Parse mapping docs for Milestone 1–3 requirements.  
3. For each requirement:
   - Confirm implementation in code (✅/⚠️/❌).
   - Confirm tests cover it (with unpacked fixtures).  
4. Spot missing features, test mismatches, dead/import-error code, lint issues.  
5. Verify imports & API usage from Memory API and Core Packages.  
6. Propose any needed unit tests (with snapshot paths).  
7. Generate unified-diff patches for fixes or cleanup—**against the unpacked code**.  
8. Summarize with checklist, “Issues & Gaps,” and `diff --git` patches.


#### 5. Gateway Service

You’re an expert in our Python/FastAPI + ArangoDB memory system. You have:

1. All mapping & milestone docs:
   - `requirements_to_milestone_mapping.md`
   - `project_development_milestones.md`
2. The existing unit tests under:
   - `tests/unit/services/gateway/`
3. **Attached tarball** `batvault_live_snapshot.tar`—this is **your** source code + fixtures. **Unpack and use exclusively.**

**Task for “Gateway” (Milestones 1–3)**  
1. **Unpack** `batvault_live_snapshot.tar`.  
2. Identify applicable Milestone 1–3 requirements from mapping docs.  
3. For each requirement:
   - Confirm code implements it (✅/⚠️/❌).
   - Confirm tests cover it (using unpacked fixtures).  
4. Spot missing/partial features, test mismatches, dead or import-error code, lint issues.  
5. Verify imports & API usage from API Edge, Memory API, and Core Packages.  
6. Propose any additional unit tests (with snapshot references).  
7. Generate unified-diff patches for code fixes, test stubs/fixes, or cleanup—**against unpacked code**.  
8. Summarize with checklist table, “Issues & Gaps,” and `diff --git` patches.



#### 6. Performance & Integration

You’re an expert in our Python/FastAPI + ArangoDB memory system. You have:

1. All mapping & milestone docs:
   - `requirements_to_milestone_mapping.md`
   - `project_development_milestones.md`
2. The performance tests under:
   - `tests/performance/`
3. The integration & ops tests under:
   - `tests/integration/`
   - `tests/ops/`
4. **Attached tarball** `batvault_live_snapshot.tar`—unpack and use as the sole source of code & fixtures.

**Task for “Performance & Integration” (Milestones 1–3)**  
1. **Unpack** `batvault_live_snapshot.tar`.  
2. Parse mapping docs for non-functional, performance, and integration requirements.  
3. For each requirement:
   - Confirm tests exist and cover targets (✅/⚠️/❌) with unpacked fixtures.
   - Spot missing performance checks or integration flows.  
4. Spot dead tests, import or lint issues, misconfigured harnesses.  
5. Propose missing integration or performance tests (with snapshot paths).  
6. Generate unified-diff patches to add missing tests or fix broken ones—**against unpacked code**.  
7. Summarize with checklist table, “Issues & Gaps,” and `diff --git` patches.

