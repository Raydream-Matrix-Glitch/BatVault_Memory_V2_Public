services:

  # ---- Monitoring Stack ---------------------------------------------------
  prometheus:
    image: prom/prometheus:latest
    restart: unless-stopped
    command: ["--config.file=/etc/prometheus/prometheus.yml", "--web.enable-lifecycle"]
    volumes:
      - ./ops/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./ops/prometheus/alerts.yml:/etc/prometheus/alerts.yml:ro
    ports: ["9090:9090"]
    networks: [batnet]

  grafana:
    image: grafana/grafana:10.4.2
    restart: unless-stopped
    environment:
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Viewer
      - GF_LOG_FILTERS=authn.service:info
    volumes:
      - ./ops/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources:ro
      - ./ops/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./ops/grafana/dashboards:/var/lib/grafana/dashboards:ro
    ports: ["3000:3000"]
    depends_on: [prometheus]
    networks: [batnet]

  arangodb:
    image: arangodb:3.12.4
    restart: always
    environment:
      - ARANGO_ROOT_PASSWORD=${ARANGO_ROOT_PASSWORD:-batvault}
    command: >
      arangod
      --server.endpoint=tcp://0.0.0.0:8529
      --server.authentication=false
      --experimental-vector-index
    ports:
      - "8529:8529"
    volumes:
      - ./docker-data/arangodb:/var/lib/arangodb3
    networks: [batnet]

  bootstrap:
    build:
      context: .
      dockerfile: ops/docker/Dockerfile.bootstrap
      args:
        SERVICE_NAME:
    depends_on:
      arangodb:
        condition: service_started
    volumes:
      - ./:/app
    working_dir: /app
    entrypoint: ["python", "ops/bootstrap.py"]
    restart: "no"
    networks: [batnet]

  redis:
    image: redis:7-alpine
    restart: unless-stopped
    ports: ["6379:6379"]
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks: [batnet]

  minio:
    image: minio/minio:latest
    restart: unless-stopped
    command: server /data --console-address ":9001"
    environment:
      - MINIO_ROOT_USER=${MINIO_ACCESS_KEY:-minioadmin}
      - MINIO_ROOT_PASSWORD=${MINIO_SECRET_KEY:-minioadmin}
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ./docker-data/minio:/data
    healthcheck:
      test: ["CMD", "bash", "-lc", "curl -s http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5
      disable: false
    networks: [batnet]

  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.103.1
    restart: unless-stopped
    command: ["--config=/etc/otelcol/config.yaml"]
    volumes:
      - ./ops/otel/otel-collector-config.yaml:/etc/otelcol/config.yaml:ro
    ports:
      - "4317:4317"
      - "4318:4318"
      - "13133:13133"
    networks: [batnet]

  api_edge:
    build:
      context: .
      dockerfile: ops/docker/Dockerfile.python
      args:
        SERVICE_NAME: api_edge
    environment:
      - PYTHONUNBUFFERED=1
      - OTEL_EXPORTER_OTLP_ENDPOINT=${OTEL_EXPORTER_OTLP_ENDPOINT:-http://otel-collector:4317}
      - OTEL_SERVICE_NAME=api_edge
      - BATVAULT_HEALTH_PORT=8080
    env_file: [.env]
    command: ["python", "-m", "api_edge.__main__"]
    ports: ["8080:8080"]
    healthcheck:
      test: ["CMD","curl","-fsS","http://localhost:8080/healthz"]
      interval: 10s
      timeout: 5s
      start_period: 15s
      retries: 5
    depends_on:
      gateway:
        condition: service_healthy
      redis:
        condition: service_healthy
      minio:
        condition: service_healthy
    restart: unless-stopped
    networks: [batnet]

  gateway:
    build:
      context: .
      dockerfile: ops/docker/Dockerfile.python
      args:
        SERVICE_NAME: gateway
    environment:
      - PYTHONUNBUFFERED=1
      - OTEL_EXPORTER_OTLP_ENDPOINT=${OTEL_EXPORTER_OTLP_ENDPOINT:-http://otel-collector:4317}
      - OTEL_SERVICE_NAME=gateway
      - BATVAULT_HEALTH_PORT=8081
    env_file: [.env]
    command: ["python", "-m", "gateway.__main__"]
    ports: ["8081:8081"]
    healthcheck:
      test: ["CMD","curl","-fsS","http://localhost:8081/healthz"]
      interval: 10s
      timeout: 5s
      start_period: 15s
      retries: 5
    depends_on:
      memory_api:
        condition: service_healthy
      redis:
        condition: service_healthy
      minio:
        condition: service_healthy
    restart: unless-stopped
    networks: [batnet]

  memory_api:
    build:
      context: .
      dockerfile: ops/docker/Dockerfile.python
      args:
        SERVICE_NAME: memory_api
    environment:
      - PYTHONUNBUFFERED=1
      - OTEL_EXPORTER_OTLP_ENDPOINT=${OTEL_EXPORTER_OTLP_ENDPOINT:-http://otel-collector:4317}
      - OTEL_SERVICE_NAME=memory-api
      - ARANGO_VECTOR_INDEX_ENABLED=true
      - BATVAULT_HEALTH_PORT=8000
    env_file: [.env]
    command: ["python", "-m", "memory_api.__main__"]
    # Expose container:8000 to host:8082 (internal still 8000)
    ports: ["8082:8000"]
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8000/healthz"]
      interval: 10s
      start_period: 15s
      retries: 5
    depends_on:
      redis:
        condition: service_healthy
      minio:
        condition: service_healthy
    restart: unless-stopped
    networks: [batnet]

  # -------------------------------------------------------------------
  # Open-source LLM & Embeddings services (optional via profile: llm)
  # These are reachable on the Docker network; published ports are for local dev only.
  vllm-control:
    profiles: [llm]
    image: vllm/vllm-openai:latest
    environment:
      - VLLM_MODEL_NAME=${VLLM_MODEL_NAME:-mistral-7b-instruct}
      - HF_HOME=/data/hf
    command:
      - python
      - -m
      - vllm.entrypoints.openai.api_server
      - --model
      - "${VLLM_MODEL_NAME:-mistral-7b-instruct}"
    ports:
      - "8000:8000"
    healthcheck:
      test: ["CMD-SHELL", "curl -fs http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
    volumes:
      - models-cache:/data/hf
    networks: [batnet]

  tgi-canary:
    profiles: [llm]
    image: ghcr.io/huggingface/text-generation-inference:latest
    environment:
      - MODEL_ID=${TGI_MODEL_ID:-llama-3.1-8b-instruct}
      - NUM_PROMPT_TOKENS=1024
      - NUM_RESPONSE_TOKENS=1024
      - HF_HOME=/data/hf
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN:-}
    command:
      - text-generation-launcher
      - --model-id
      - "${TGI_MODEL_ID:-llama-3.1-8b-instruct}"
      - --port
      - "8080"
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD-SHELL", "curl -fs http://localhost:8080/health || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
    volumes:
      - models-cache:/data/hf
    networks: [batnet]

  tei-embed:
    profiles: [llm]
    image: ghcr.io/huggingface/text-embeddings-inference:latest
    environment:
      - MODEL_NAME=${TEI_MODEL_NAME:-bge-base-en-v1.5}
      - HF_HOME=/data/hf
    command:
      - python
      - -m
      - tei_server
      - --model
      - "${TEI_MODEL_NAME:-bge-base-en-v1.5}"
      - --port
      - "8085"
    ports:
      - "8085:8085"
    healthcheck:
      test: ["CMD-SHELL", "curl -fs http://localhost:8085/health || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
    volumes:
      - models-cache:/data/hf
    networks: [batnet]

  ingest:
    build:
      context: .
      dockerfile: ops/docker/Dockerfile.python
      args:
        SERVICE_NAME: ingest
    environment:
      - PYTHONUNBUFFERED=1
      - OTEL_EXPORTER_OTLP_ENDPOINT=${OTEL_EXPORTER_OTLP_ENDPOINT:-http://otel-collector:4317}
      - OTEL_SERVICE_NAME=ingest
      - ARANGO_VECTOR_INDEX_ENABLED=true
      - BATVAULT_HEALTH_PORT=8083
    env_file: [.env]
    command: ["python", "-m", "ingest.__main__"]
    ports: ["8083:8083"]
    healthcheck:
      test: ["CMD","curl","-fsS","http://localhost:8083/healthz"]
      interval: 10s
      timeout: 5s
      start_period: 15s
      retries: 5
    depends_on:
      - memory_api
      - minio
      - otel-collector
    restart: unless-stopped
    volumes:
      - ./memory/fixtures:/app/memory/fixtures:ro
    networks: [batnet]

  jaeger:
    image: jaegertracing/all-in-one:1.57
    restart: unless-stopped
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    ports:
      - "16686:16686"
      - "14250:14250"
      - "14268:14268"
    networks: [batnet]

  frontend:
    build:
      context: ./batvault_frontend
      dockerfile: dockerfile
    ports:
      - "5173:80"
    depends_on:
      api_edge:
        condition: service_healthy
    networks: [batnet]
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -fs http://localhost || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5

volumes:
  models-cache:

networks:
  batnet:
    driver: bridge
