# Gateway Decision Query System

## Overview

This document provides a "view from the cockpit" of how the Gateway system processes decision queries today (with milestones 0-3 complete). 

## Example Query Flow

**Query:** "Why did we make the decision to panasonic-exit-plasma-2012?"

### 1. Processing Pipeline

The Gateway processes your question through the following stages (p95 ≤ 3s):

| Stage | What it does | Key details |
|-------|--------------|-------------|
| **resolve** | Recognises the slug (unique identifier like "panasonic-exit-plasma-2012"), so it skips text search and treats it as the anchor decision | milliseconds |
| **plan → exec graph** | Calls the Memory API to walk one hop (k = 1) around the anchor. This returns:<br/>• 3 supporting events (pan-e2,pan-e3,pan-e11)<br/>• 2 transitions (one incoming, one outgoing) | Finds related decisions and events connected to the main decision |
| **enrich** | Fetches compact "envelopes" (standardized data packages) for each neighbour so the gateway is schema-agnostic | Gets detailed information for each related item |
| **bundle** | Packs everything into a WhyDecisionEvidence@1 bundle. The set allowed_ids must equal the union of anchor + events + present transitions | Creates a complete package of all relevant information |
| **size check** | If the serialised JSON > 8 192 bytes, the selector model drops the lowest-scoring items, but never below anchor + 1 item thanks to the constants:<br/>`MAX_PROMPT_BYTES = 8192`<br/>`SELECTOR_TRUNCATION_THRESHOLD = 6144` | Ensures the data package isn't too large while keeping essential information |
| **prompt build** | Wraps the bundle in a canonical prompt envelope and fingerprints it (SHA-256) for audit | Creates a standardized format and unique identifier for tracking |
| **answer** | Because the LLM (Large Language Model) isn't wired in until milestone 4, the templater synthesises a short answer from the anchor's rationale plus the highest-ranked evidence item. | Generates the actual response using rule-based templates |
| **validate** | Confirms the output matches WhyDecisionAnswer@1: ≤ 320 chars, cites only allowed_ids, and always includes the anchor ID. On failure it retries twice, then falls back to a deterministic template. | Quality control to ensure the answer meets requirements |
| **render** | Returns a structured JSON blob (WhyDecisionResponse@1) and streams the short_answer field to the client. All other fields arrive at once. | Packages and delivers the final response |
| **log & store** | A signed "generic log envelope" with timing, IDs, and bundle metrics is written to MinIO/S3 for replay/audit | Saves everything for future reference and compliance |

### 2. Response Structure

```jsonc
{
  "intent": "why_decision",
  "evidence": {
    "anchor": { /* decision envelope - the main decision being explained */ },
    "events": [ /* 3 events that influenced this decision */ ],
    "transitions": {
      "preceding": [{ /* trans-pan-2010-2012 - what led to this decision */ }],
      "succeeding": [{ /* trans-pan-2012-2014 - what happened after */ }]
    },
    "allowed_ids": [
      "panasonic-exit-plasma-2012",
      "pan-e2", "pan-e3", "pan-e11",
      "trans-pan-2010-2012", "trans-pan-2012-2014"
    ]
  },
  "answer": {
    "short_answer": "Panasonic quit plasma TV production in 2012 after the division posted ¥913 m losses and LCD demand surged; leadership redirected funds toward EV-battery and automotive-electronics growth.",
    "supporting_ids": ["panasonic-exit-plasma-2012", "pan-e2"]
  },
  "completeness_flags": {
    "has_preceding": true,
    "has_succeeding": true,
    "event_count": 3
  },
  "meta": {
    "policy_id": "why_v1",
    "prompt_id": "templater_v1",
    "prompt_fingerprint": "sha256:…",
    "snapshot_etag": "sha256:…",
    "retries": 0,
    "latency_ms": 980,
    "fallback_used": false
  }
}
```

**Note:** In milestone 3 the `short_answer` always comes from the templater (rule-based system), so `fallback_used` is normally false—the LLM path is simply disabled.

### 3. Key Features

#### Current Benefits (Milestones 0-3)

- **All data is audited** - Every byte that influenced the answer is captured and can be re-played later for verification
- **Schema-agnostic** - If you add new fields tomorrow, they flow through automatically because the gateway relies on the live Field Catalog (dynamic data structure definitions)
- **Size-safe** - Answers never crash the model later because the bundle is hard-capped at 8 KB and shrinks intelligently if needed
- **Fast & deterministic** - No heavy model calls yet, so P95 latency (95th percentile response time) stays under 1 s for slug queries

#### Understanding the Response

- **`completeness_flags`** - Let you know whether earlier or later causal links existed in the decision timeline
- **`evidence`** - Contains all the context used to generate the answer (the "source material")
- **`allowed_ids`** - Lists all valid IDs that can be cited in the response (prevents hallucination)
- **`meta`** - Provides audit trail and performance metrics for transparency
- **Raw evidence and all artefacts** - Reconstructible from the fingerprints in meta (full traceability)

#### Future Evolution

When milestone 4 connects the JSON-only micro-LLM (specialized language model), the same pipeline will reuse this evidence bundle and simply swap the templater for a model-generated `short_answer`—the contract and audit trail stay exactly the same.

## TL;DR for Non-Tech Readers

Here's what happens under the hood when you ask "Why did Panasonic exit plasma?":

1. **Find the decision** - Locate the specific decision record
2. **Pull the immediate context** - Gather related events and timeline
3. **Package it neatly (with size limits)** - Organize information efficiently
4. **Pop out a concise "because" statement** - Generate clear explanation
5. **Store everything for later inspection** - Save audit trail

The system ensures every answer is traceable, size-controlled, and based on verified data relationships.