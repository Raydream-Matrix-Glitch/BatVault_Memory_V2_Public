Summary of changes

Resolver TTL default is now 300s (env‑overridable).
Resolver search fields updated to search title | rationale | notes | tags (not content).
Vector search flag stubbed: payload can pass use_vector=true and a query_vector (list[float]). If ENABLE_EMBEDDINGS=true and ARANGO_VECTOR_INDEX_ENABLED=true, we attempt a vector query; else we transparently fall back to BM25/LIKE. No hidden embedding step.
Budgets enforced: resolve_text capped at ≤ 800 ms and expand_candidates at ≤ 250 ms via asyncio.wait_for(asyncio.to_thread(...)).
OTel spans: added across resolve, expand, schema and enrich endpoints (no exporter change required; still optional).
Gateway /v2/query HTTP timeout tightened to 0.8s.
Tests: added unit/contract tests focusing on expand_candidates happy path + edge case.


packages/core_config/src/core_config/settings.py (TTL bump)
diff
Copy
Edit
--- packages/core_config/src/core_config/settings.py
+++ packages/core_config/src/core_config/settings.py
@@ -49,6 +49,10 @@
     arango_meta_collection: str = Field(default="meta", alias="ARANGO_META_COLLECTION")
 
     # Redis
+    cache_ttl_expand_sec: int = Field(default=60, alias="CACHE_TTL_EXPAND_SEC")
+    cache_ttl_resolve_sec: int = Field(default=300, alias="CACHE_TTL_RESOLVE_SEC")
+
     redis_url: str = Field(default="redis://redis:6379/0", alias="REDIS_URL")
packages/core_logging/src/core_logging/__init__.py (export tracing helper)
diff
Copy
Edit
--- packages/core_logging/src/core_logging/__init__.py
+++ packages/core_logging/src/core_logging/__init__.py
@@ -1 +1,3 @@
 from .logger import get_logger, log_event, log_stage
+
+from .logger import trace_span
packages/core_logging/src/core_logging/logger.py (tracing helper)
diff
Copy
Edit
--- packages/core_logging/src/core_logging/logger.py
+++ packages/core_logging/src/core_logging/logger.py
@@ -66,3 +66,33 @@
     safe_extras = {k: v for k, v in extras.items() if k not in _RESERVED}
     logger.info(event, extra=safe_extras)
+
+from contextlib import contextmanager
+try:
+    from opentelemetry import trace as _otel_trace  # optional
+except Exception:  # pragma: no cover
+    _otel_trace = None
+
+@contextmanager
+def trace_span(name: str, **attrs):
+    """
+    Lightweight span wrapper. Uses OpenTelemetry if present, else no‑op.
+    """
+    if _otel_trace:
+        tracer = _otel_trace.get_tracer("batvault")
+        with tracer.start_as_current_span(name) as span:
+            for k, v in attrs.items():
+                try:
+                    span.set_attribute(k, v)
+                except Exception:
+                    pass
+            yield span
+            return
+    # Fallback no‑op
+    yield None
packages/core_storage/src/core_storage/arangodb.py
(Resolver multi‑field, vector stub, cache key, and expand retained)

diff
Copy
Edit
--- packages/core_storage/src/core_storage/arangodb.py
+++ packages/core_storage/src/core_storage/arangodb.py
@@
-    def resolve_text(self, q: str, limit: int = 10) -> dict:
+    def resolve_text(self, q: str, limit: int = 10, use_vector: bool = False, query_vector: list[float] | None = None) -> dict:
@@
-        key = f"resolve:{hash(q)}:l{limit}"
+        key = f"resolve:{hash((q, bool(use_vector)))}:l{limit}"
         cached = self._cache_get(key)
         if cached:
             return cached
-        results = []
+        results = []
+        settings = get_settings()
+        # Optional vector search branch
+        if use_vector and settings.enable_embeddings and os.getenv('ARANGO_VECTOR_INDEX_ENABLED','false').lower() == 'true':
+            try:
+                if not query_vector:
+                    raise Exception('query_vector required when use_vector=true')
+                aql = (
+                    "FOR d IN nodes FILTER HAS(d,'embedding') "
+                    "LET dist = COSINE_DISTANCE(d.embedding, @qv) "
+                    "SORT dist ASC LIMIT @limit "
+                    "RETURN {id: d._key, score: 1.0 - dist, title: d.title, type: d.type}"
+                )
+                cursor = self.db.aql.execute(aql, bind_vars={"qv": query_vector, "limit": limit})
+                results = list(cursor)
+                resp = {"query": q, "matches": results, "vector_used": True}
+                self._cache_set(key, resp, get_settings().cache_ttl_resolve_sec)
+                return resp
+            except Exception:
+                # fall through to BM25/LIKE
+                pass
@@
-            aql = (
-                "FOR d IN nodes_search "
-                "SEARCH ANALYZER(PHRASE(d.content, @q), 'text_en') "
-                "SORT BM25(d) DESC LIMIT @limit RETURN {id: d._key, score: BM25(d), title: d.title, type: d.type}"
-            )
+            aql = (
+                "FOR d IN nodes_search "
+                "SEARCH ANALYZER( "
+                "  PHRASE(d.title, @q) OR PHRASE(d.rationale, @q) OR "
+                "  PHRASE(d.notes, @q) OR PHRASE(d.tags, @q), "
+                "  'text_en' "
+                ") "
+                "SORT BM25(d) DESC LIMIT @limit "
+                "RETURN {id: d._key, score: BM25(d), title: d.title, type: d.type}"
+            )
             cursor = self.db.aql.execute(aql, bind_vars={"q": q, "limit": limit})
             results = list(cursor)
         except Exception:
             # Fallback naive LIKE on nodes collection
-            aql = (
-                "FOR d IN nodes FILTER LIKE(LOWER(d.content), LOWER(CONCAT('%', @q, '%'))) "
-                "LIMIT @limit RETURN {id: d._key, score: 0.0, title: d.title, type: d.type}"
-            )
+            aql = (
+                "FOR d IN nodes "
+                "FILTER LIKE(LOWER(d.title), LOWER(CONCAT('%', @q, '%'))) "
+                "  OR LIKE(LOWER(d.rationale), LOWER(CONCAT('%', @q, '%'))) "
+                "  OR LIKE(LOWER(d.notes), LOWER(CONCAT('%', @q, '%'))) "
+                "  OR (IS_LIST(d.tags) AND @q IN d.tags) "
+                "LIMIT @limit RETURN {id: d._key, score: 0.0, title: d.title, type: d.type}"
+            )
             cursor = self.db.aql.execute(aql, bind_vars={"q": q, "limit": limit})
             results = list(cursor)
-        resp = {"query": q, "matches": results}
+        resp = {"query": q, "matches": results, "vector_used": False}
         self._cache_set(key, resp, get_settings().cache_ttl_resolve_sec)
         return resp
Note: vector branch relies on driver support for COSINE_DISTANCE with a vector index. If your Arango version uses a different function name, I’ll adjust. Fallback remains BM25/LIKE.

(Your expand_candidates(...) implementation from the previous patch is unchanged and kept.)

services/memory_api/src/memory_api/app.py
(Async handlers + budgets + vector params + spans for enrich/schema)

diff
Copy
Edit
--- services/memory_api/src/memory_api/app.py
+++ services/memory_api/src/memory_api/app.py
@@
-from core_storage import ArangoStore
-import httpx
+from core_storage import ArangoStore
+import httpx
+import asyncio
@@
-@app.post("/api/resolve/text")
-def resolve_text(payload: dict, response: Response):
-    from core_logging import trace_span
-    q = payload.get('q', '')
-    if not q:
-        return {"matches": [], "query": q}
-    st = store()
-    with trace_span("memory.resolve_text", q=q):
-        doc = st.resolve_text(q, limit=int(payload.get('limit', 10)))
-    etag = st.get_snapshot_etag()
-    if etag:
-        response.headers["x-snapshot-etag"] = etag
-    log_stage(logger, "resolver", "text_resolved", request_id=payload.get("request_id"), snapshot_etag=etag, match_count=len(doc.get("matches", [])))
-    return doc
+@app.post("/api/resolve/text")
+async def resolve_text(payload: dict, response: Response):
+    from core_logging import trace_span
+    q = payload.get("q", "")
+    use_vector = bool(payload.get("use_vector", False))
+    query_vector = payload.get("query_vector")
+    if not q and not (use_vector and query_vector):
+        return {"matches": [], "query": q, "vector_used": False}
+    st = store()
+    async def _work():
+        return st.resolve_text(q, limit=int(payload.get("limit", 10)), use_vector=use_vector, query_vector=query_vector)
+    with trace_span("memory.resolve_text", q=q, use_vector=use_vector):
+        doc = await asyncio.wait_for(asyncio.to_thread(_work), timeout=0.8)
+    etag = st.get_snapshot_etag()
+    if etag:
+        response.headers["x-snapshot-etag"] = etag
+    log_stage(logger, "resolver", "text_resolved",
+              request_id=payload.get("request_id"),
+              snapshot_etag=etag,
+              match_count=len(doc.get("matches", [])),
+              vector_used=doc.get("vector_used"))
+    return doc
@@
-@app.post("/api/graph/expand_candidates")
-def expand_candidates(payload: dict, response: Response):
-    from core_logging import trace_span
-    anchor = payload.get('anchor')
-    k = int(payload.get('k', 1))
-    if not anchor:
-        raise HTTPException(status_code=400, detail="anchor is required")
-    st = store()
-    with trace_span("memory.expand_candidates", anchor=anchor, k=k):
-        doc = st.expand_candidates(anchor, k=k)
-    etag = st.get_snapshot_etag()
-    if etag:
-        response.headers["x-snapshot-etag"] = etag
-    log_stage(logger, "expand", "candidates_returned", request_id=payload.get("request_id"), snapshot_etag=etag, neighbor_count=len(doc.get("neighbors", [])))
-    return doc
+@app.post("/api/graph/expand_candidates")
+async def expand_candidates(payload: dict, response: Response):
+    from core_logging import trace_span
+    anchor = payload.get("anchor")
+    k = int(payload.get("k", 1))
+    if not anchor:
+        raise HTTPException(status_code=400, detail="anchor is required")
+    st = store()
+    async def _work():
+        return st.expand_candidates(anchor, k=k)
+    with trace_span("memory.expand_candidates", anchor=anchor, k=k):
+        doc = await asyncio.wait_for(asyncio.to_thread(_work), timeout=0.25)
+    etag = st.get_snapshot_etag()
+    if etag:
+        response.headers["x-snapshot-etag"] = etag
+    log_stage(logger, "expand", "candidates_returned",
+              request_id=payload.get("request_id"),
+              snapshot_etag=etag,
+              neighbor_count=len(doc.get("neighbors", [])))
+    return doc
@@  # add spans to schema/enrich endpoints
 @app.get("/api/schema/fields")
-def get_field_catalog(response: Response):
+def get_field_catalog(response: Response):
+    from core_logging import trace_span
+    with trace_span("memory.schema_fields"):
     ...
 @app.get("/api/schema/relations")
-def get_relation_catalog(response: Response):
+def get_relation_catalog(response: Response):
+    from core_logging import trace_span
+    with trace_span("memory.schema_relations"):
     ...
 @app.get("/api/enrich/decision/{node_id}")
-def enrich_decision(node_id: str, response: Response):
+def enrich_decision(node_id: str, response: Response):
+    from core_logging import trace_span
+    with trace_span("memory.enrich_decision", node_id=node_id):
     ...
 @app.get("/api/enrich/transition/{node_id}")
-def enrich_transition(node_id: str, response: Response):
+def enrich_transition(node_id: str, response: Response):
+    from core_logging import trace_span
+    with trace_span("memory.enrich_transition", node_id=node_id):
     ...
Timeouts note: wait_for enforces service budgets; the underlying AQL call may continue server‑side. If you want hard DB‑level caps, I can add Arango AQL options (maxRuntime) in a follow‑up.

services/gateway/src/gateway/app.py (budget for /v2/query)
diff
Copy
Edit
--- services/gateway/src/gateway/app.py
+++ services/gateway/src/gateway/app.py
@@
-    resp = httpx.post(f"{settings.memory_api_url}/api/resolve/text", json=payload, timeout=5.0)
+    resp = httpx.post(f"{settings.memory_api_url}/api/resolve/text", json=payload, timeout=0.8)
New tests
Unit tests for expand happy/edge paths

services/memory_api/tests/test_expand_candidates_unit.py

python
Copy
Edit
import types
import pytest
from memory_api.app import store

class DummyCursor:
    def __init__(self, items): self._it = iter(items)
    def __iter__(self): return self
    def __next__(self): return next(self._it)

def test_expand_happy_path(monkeypatch):
    st = store()
    class DummyDB:
        class AQL:
            @staticmethod
            def execute(aql, bind_vars=None):
                doc = {
                    "anchor": {"_key":"A1","type":"decision","title":"Anchor"},
                    "neighbors": [
                        {"node":{"_key":"N1","type":"event","title":"E1"}, "edge":{"relation":"preceded_by","timestamp":"2011"}},
                        {"node":{"_key":"N2","type":"event","title":"E2"}, "edge":{"relation":"succeeded_by","timestamp":"2013"}}
                    ]
                }
                return DummyCursor([doc])
        aql = AQL()
    st.db = DummyDB()
    res = st.expand_candidates("A1", k=1)
    assert res["anchor"]["_key"] == "A1"
    assert len(res["neighbors"]) == 2
    assert {"id","type","title","edge"} <= set(res["neighbors"][0].keys())

def test_expand_missing_anchor(monkeypatch):
    st = store()
    class DummyDB:
        class AQL:
            @staticmethod
            def execute(aql, bind_vars=None):
                return DummyCursor([{"anchor": None, "neighbors": []}])
        aql = AQL()
    st.db = DummyDB()
    res = st.expand_candidates("does-not-exist", k=1)
    assert res["anchor"] is None
    assert res["neighbors"] == []
These complement the earlier contract tests and exercise both happy‑path and “no anchor” edge case without requiring a live Arango instance.