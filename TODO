### Follow-ups (recommended) @observability

## 1
Remove legacy ttfb_ms after dashboards are verified (avoid duplicate storage).

Consider normalizing route labels (rather than raw path) if you want per-endpoint error/latency panels without high cardinality.

If you want K8s-ready, I can translate this to Helm charts (Prometheus Operator CRDs + Grafana dashboards ConfigMaps), still env-driven.

## 2
Introducing Redis
Redis is a popular “in-memory” store (our cookie jar) that sits alongside the application. It lets us save small bits of data—like that snapshot ID—for a short time (say, one minute). When the ingest service wants the snapshot ID:

It first checks Redis (“Is there one in the jar?”).
If yes, that’s a cache hit—we grab it instantly.
If no, that’s a cache miss—we go back to scanning all the files.
After a cache miss, once we’ve computed the ID, we store it in Redis so the next request will be a hit.

Recording hits and misses
By counting how many hits and misses happen, we learn how effective our caching is. For example:
90 hits and 10 misses in a minute tells us we saved ourselves 90 full rescans!
We expose those counts (“cache_hit_total” and “cache_miss_total”) to our monitoring dashboard so we can spot if something’s wrong (e.g., if every request is a miss, maybe the cache isn’t working).

Does “ingest” already talk to Redis?
Not yet. In the current codebase, the ingest service doesn’t use Redis. The change I proposed adds just a small bit of logic:
Try to connect to Redis (using the address we’ve configured elsewhere).
If Redis is available, use it as a cache.
If not, just skip caching and fall back to the old behavior.