# Gateway LLM configuration

# Primary inference endpoint for the control summariser (vLLM)
CONTROL_MODEL_ENDPOINT=http://vllm-control:8000

# Canary inference endpoint (TGI).  A fraction of requests will be routed
# here based on a stable hash of the request ID unless the override
# header is present.
CANARY_MODEL_ENDPOINT=http://tgi-canary:8080

# Percentage of requests (0â€“100) that should be sent to the canary.
CANARY_PCT=10

# Name of the HTTP header that forces canary routing when present.
CANARY_HEADER_OVERRIDE=x-batvault-canary

# Generation parameters applied to both models.  Temperature should
# remain zero for deterministic summarisation.  Max tokens caps the
# generated JSON length.
LLM_TEMPERATURE=0.0
LLM_MAX_TOKENS=512